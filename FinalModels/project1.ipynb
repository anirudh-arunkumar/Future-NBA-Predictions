{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import Lasso\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameDataframe = pd.read_csv(\"game.csv\")\n",
    "# print(gameDataframe)\n",
    "gameDataframe['game_date'] = pd.to_datetime(gameDataframe['game_date'])\n",
    "\n",
    "# Filter the data to only include rows from 2000 onwards\n",
    "filtered_df = gameDataframe[gameDataframe['game_date'].dt.year >= 2000]\n",
    "# print(filtered_df)\n",
    "\n",
    "filtered_df['wl_home'] = filtered_df['wl_home'].map({'W': 1, 'L': 0})\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "non_numeric_columns = filtered_df.select_dtypes(exclude=[np.number]).columns\n",
    "filtered_df = filtered_df.select_dtypes(include=[np.number])\n",
    "\n",
    "correlations = filtered_df.corr()['wl_home'].abs().sort_values(ascending=False)\n",
    "\n",
    "sns.heatmap(filtered_df.corr())\n",
    "plt.show()\n",
    "\n",
    "remainingFeatures = correlations[correlations > .15].index #indexes of all features with a correlations above 15 percent towards the win or loss of a home team\n",
    "# print(remainingFeatures)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "reducedData = filtered_df[remainingFeatures].drop(columns=['wl_home'])\n",
    "scaledData = scaler.fit_transform(reducedData)\n",
    "pca = PCA()\n",
    "# pca.fit(scaledData)\n",
    "pcaData = pca.fit_transform(scaledData)\n",
    "\n",
    "percentVar = np.round(pca.explained_variance_ratio_*100,decimals= 1)\n",
    "labels = ['PC' + str(x) for x in range(1,len(percentVar) + 1)]\n",
    "plt.bar(x = range(1,len(percentVar)+1),height=percentVar,tick_label = labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pca_df = pd.DataFrame(pcaData[:,:10], columns=[f'PC{i+1}' for i in range(10)]) #top 10 principal components account for ~90% variance \n",
    "pca_df['wl_home'] = filtered_df['wl_home']\n",
    "correlations_with_win = pca_df.corr()['wl_home'].drop('wl_home').sort_values(ascending=False)\n",
    "# print(filtered_df['wl_home'])\n",
    "# print(pca_df)\n",
    "print(correlations_with_win)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the Scree plot give us insight towards the Principal Components that contribute the highest percentage of variance in general. It's important to note that this is general variance, not variance directly pertaining towards the win or loss of the home team. \n",
    "\n",
    "Conversely, the printed correlations are tied directly to the win or loss of the home team for each Principal Component. As we can see, the most explanatory principal component is PC1, which is negatively correlated with the win or loss of the home team. Meaning, if there is a high value for PC1 in a given game, that game lends itself to being more likely to having been a loss for the home team. While other factors such as PC2 and PC6 are among the higher correlated PC's with W/L, their correlation contstant is not nearly as dominant as PC1's.\n",
    "\n",
    "It's important to note, this is data collected after the fact. Since our model is looking to predict the outcome of games, these statistics can't necessarily help us as we have no indication as to how these features will play out in a given game. However, there is some merit into observing the cumulative of these values leading up to each game. If a team is consistently putting up high contributions towards the PC1 feature, for example, we could therefore assume it is less likely they will win their following game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = gameDataframe[gameDataframe['game_date'].dt.year >= 2023]\n",
    "# print(filtered_df)\n",
    "filtered_df['wl_home'] = filtered_df['wl_home'].map({'W': 1, 'L': 0})\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "filtered_df = filtered_df.sort_values(by='game_date')\n",
    "# hometeam_df = filtered_df.groupby('team_name_home')\n",
    "\n",
    "mean_last_10_games = (\n",
    "    filtered_df.groupby('team_name_home')\n",
    "    .apply(lambda x: x.tail(10).select_dtypes(include='number').mean())  # Get the mean of the last 10 games per team\n",
    "    .drop(columns=['season_id','team_id_home','game_id'])\n",
    "    .reset_index(drop = True)\n",
    "    # .drop(columns=['wl_home'])  # Optional: Drop 'wl_home' if it's not needed in the result\n",
    ")\n",
    "\n",
    "# print(mean_last_10_games)\n",
    "correlations = mean_last_10_games.corr()['wl_home'].sort_values(ascending=False)\n",
    "# print(correlations)\n",
    "# remainingFeatures = correlations[correlations > .15].index #indexes of all features with a correlations above 15 percent towards the win or loss of a home team\n",
    "# print(remainingFeatures)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# reducedData = mean_last_10_games.drop(columns=['wl_home'])\n",
    "reducedData = reducedData.dropna()\n",
    "scaledData = scaler.fit_transform(reducedData)\n",
    "pca = PCA()\n",
    "pcaData = pca.fit_transform(scaledData)\n",
    "\n",
    "percentVar = np.round(pca.explained_variance_ratio_*100,decimals= 1)\n",
    "labels = ['PC' + str(x) for x in range(1,len(percentVar) + 1)]\n",
    "plt.bar(x = range(1,len(percentVar)+1),height=percentVar,tick_label = labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('Scree Plot')\n",
    "\n",
    "pca_df = pd.DataFrame(pcaData, columns=[f'PC{i+1}' for i in range(pcaData.shape[1])]) #top 10 principal components account for ~90% variance \n",
    "pca_df['wl_home'] = mean_last_10_games['wl_home']\n",
    "# print(pca_df)\n",
    "pcacorrelations_with_win = pca_df.corr()['wl_home'].drop('wl_home').sort_values(ascending=False)\n",
    "# print(filtered_df['wl_home'])\n",
    "# print(pca_df)\n",
    "# print(pcacorrelations_with_win)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(pcacorrelations_with_win.index, pcacorrelations_with_win.values, color='skyblue')\n",
    "plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "plt.title('Correlation of Principal Components with Winning')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Correlation with Winning (wl_home)')\n",
    "plt.xticks(rotation=45)  # Rotate x labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig = px.scatter_3d(pca_df, x='PC1', y='PC2', z='PC4', color='wl_home',\n",
    "                    labels={'wl_home': 'Win Percentage'},\n",
    "                    title='3D Scatter Plot of PCA Components vs Win Percentage',\n",
    "                    color_continuous_scale=px.colors.sequential.Viridis)\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_traces(marker=dict(size=5))  # Adjust marker size if needed\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis_title='Principal Component 1',\n",
    "    yaxis_title='Principal Component 2',\n",
    "    zaxis_title='Principal Component 3',\n",
    "))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion of the data reduction is much more relevant towards the scope of our project. By focusing on the mean of the features of each team from their last 10 games, we can get a good understanding of how the team is performing in general at the time of their upcoming games, as well as the most important features that are contributing towards their wins or losses. \n",
    "\n",
    "By identifying the principal components that are most directly correlated (positive or negative), we a good idea of predicting the outcomes of future games by using these principal components and seeing how they reflect a team leading up to their future games.\n",
    "\n",
    "Looking at the bar graph from the second graphic, we can see that there are several PC's that are heavily correlated with winning. PC1 and PC2 are negatively correlated with winning, and PC4 is positively correlated with winning. While not perfect, we can get a general understanding of this by plotting it in a 3-Dimensional space as graphic #3 has done. As you can see, when we move towards a higher value of PC1 and PC2, the percentage of winning goes down, and conversely when we move towards a lower value of PC1 and PC2 winning goes up. The opposite can be true of PC4. While this portion of our project is not used to draw definitive conclusions or predictions, this can establish a solid foundation for our machine learning models to use when predicting the outcome of games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emily\n",
    "\n",
    "filtered_df = gameDataframe[gameDataframe['game_date'].dt.year >= 2000].copy()\n",
    "filtered_df['wl_home'] = filtered_df['wl_home'].map({'W': 1, 'L': 0})\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "filtered_df.dropna(inplace=True)\n",
    "\n",
    "non_numeric_columns = filtered_df.select_dtypes(exclude=[np.number]).columns\n",
    "filtered_df = filtered_df.select_dtypes(include=[np.number])\n",
    "\n",
    "correlations = filtered_df.corr()['wl_home'].abs().sort_values(ascending=False)\n",
    "\n",
    "remainingFeatures = correlations[correlations > .15].index \n",
    "\n",
    "scaler = StandardScaler()\n",
    "reducedData = filtered_df[remainingFeatures]\n",
    "x_set = reducedData.drop(columns='wl_home')\n",
    "y_set = reducedData['wl_home']\n",
    "x_set_scaled = scaler.fit_transform(x_set)\n",
    "\n",
    "pca_3 = PCA(n_components=3)\n",
    "new_dataset = pca_3.fit_transform(x_set_scaled) # scaledData is just X\n",
    "\n",
    "#########\n",
    "\n",
    "clustering = DBSCAN()\n",
    "clustering.fit(new_dataset)\n",
    "print(f'Number of clusters: {len(np.unique(clustering.labels_))}')\n",
    "print(f'Number of \"outliers\": {len(np.where(clustering.labels_ == -1)[0])}')\n",
    "print(f'N: {len(x_set)}')\n",
    "\n",
    "# Do we want to delete these \"outliers\" from the dataset?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LASSO for feature selection\n",
    "(Emily - need to probably do cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_game_stats_home = filtered_df.iloc[:, 7:28].copy() # only the home numerical data\n",
    "cleaned_game_stats_home.dropna(inplace=True)\n",
    "x_set = cleaned_game_stats_home.iloc[:, 1:]\n",
    "x_set_std = (x_set-x_set.mean())/x_set.std()\n",
    "y_set = cleaned_game_stats_home.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_fit = Lasso(alpha=0.02, max_iter=10000)\n",
    "lasso_fit.fit(x_set_std, y_set)\n",
    "feats = np.where(lasso_fit.coef_ != 0)[0]\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dataset = x_set.iloc[:, feats]\n",
    "\n",
    "clustering = DBSCAN(eps=0.5, min_samples=3)\n",
    "clustering.fit(selected_dataset)\n",
    "print(f'Number of clusters: {len(np.unique(clustering.labels_))}')\n",
    "print(f'Number of \"outliers\": {len(np.where(clustering.labels_ == -1)[0])}')\n",
    "print(f'N: {len(x_set)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4641_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
